
代码结果：
main.py
    -运行参数：parser
    -模型参数：config
    -dataset = load_data(args, custom)  # 返回一个加载了运行配置的DatasetLocal类
    -dataset.load(config)  # 根据模型配置加载数据
    -print_config(config)  # 输出配置
    for run_id in range(config['multirun']):  # 模型训练
        -main(args, config, logger, run_id, dataset)  # 模型运行
            T = Trainer(config=config, args= args, logger= logger)  # 根据参数创建训练器
            for epoch in range(1,51):  # 50轮
                batches = dataset.create_batches_all(config)  # 获取一个DataLoader，共176400（420*420）个图对
                for batch_pair in batches:  # 每次取128个图对
                    * batch_pair
                    data = dataset.transform_batch(batch_pair, config)  # 数据格式转换
                    * data
                    model, loss = T.train(data, model, loss_func, optimizer, target)  # 得到损失函数
                ...T.evaluation(...)  # 用验证集测试一下, 420*140=58800
                        保存各个标准（mse、p10等）最好的模型参数，共5个
            ...T.evaluation(...)  # 测试集进行评估(420+140)*140=78400，共5次，每个标准选5次里最好的
        -test_results...  # 保存实验结果
        -save_model...  # 保存模型

GSC模型
    -setup_layers()
        self.gnn_list             # 共4层GIN：29-64、64-64、64-32、32-16
        self.mlp_list_inner       # 共4层MLP：64-64、64-64、32-32、16-16
        self.mlp_list_outer       # 同上
        self.NTN                  # 共1层NTN：16-1
        self.conv_stack           # 共1层：176-88
        self.GCL_model            # GCL：64+64+32+16=176
    -forward()
        4层GIN后得到的嵌入：
            diff_rep                  # [num_graphs, 64+64+32+16], 每层图级嵌入间的差异(exp(-(g1-g2)^2))，用于得出score
            cat_node_embeddings_1     # [num_nodes, 64+64+32+16], 用于AReg
            cat_node_embeddings_2     # [num_nodes, 64+64+32+16], 用于AReg
            cat_global_embedding_1    # [num_graphs, 64+64+32+16], 用于AReg
            cat_global_embedding_2    # [num_graphs, 64+64+32+16], 用于AReg
            deepsets_outer_1          # [num_graphs, 16]GIN最后一层的图级嵌入
            deepsets_outer_2          # [num_graphs, 16]GIN最后一层的图级嵌入
        计算AReg损失
            L_cl = self.GCL_model(batch_1, batch_2, cat_node_embeddings_1, cat_node_embeddings_2, g1 = cat_global_embedding_1, g2 = cat_global_embedding_2) * self.gamma
        计算NTN
            sim_rep = self.NTN(deepsets_outer_1, deepsets_outer_2)
        计算score
            score_rep = self.conv_stack(diff_rep).squeeze()
        
GNN:
    for i in range(self.num_filter)
        gnn_list[i]                             -> conv_source_1        [num_nodes, filter[i]]
        gnn_list[i]                             -> conv_source_2        [num_nodes, d]
        Relu(mlp_list_inner[i](conv_source_1))  -> deepsets_inner_1     [num_nodes, d]
        Relu(mlp_list_inner[i](conv_source_2))  -> deepsets_inner_2     [num_nodes, d]
        add(deepsets_inner_1)                   -> deepsets_outer_1     [num_graphs, d]
        add(deepsets_inner_2)                   -> deepsets_outer_2     [num_graphs, d]
        cat                                     -> diff_rep             [num_graphs, 176]     
                                                -> cat_node_embeddings_1     [num_nodes, 176]
                                                -> cat_node_embeddings_2     
                                                -> cat_global_embedding_1    
                                                -> cat_global_embedding_2
NTN:
    NTN(deepsets_outer_1, deepsets_outer_2) -> sim_rep              [num_graphs, 16]
    sigmoid(score_sim_layer(sim_rep))       -> sim_score            [num_graphs]
score:
    conv_stack(diff_rep)                    -> score_rep            [num_graphs, 176/88]
    sigmoid(score_layer(score_rep))         -> score                [num_graphs]



mlp_list_inner[i]:
    Linear(d,d)
score_sim_layer:
    Linear(d,d)
    ReLU()
    Linear(d,1)
score_layer:
    dim = 64+64+32+16
    Linear(dim, dim/2)
    ReLU()
    Dropout(0.0)
    Linear(dim/2, dim/2)
    Dropout(0.0)
    Tanh()
        



-已改进：
1、create_batches() -> create_batches_all(), 每次获取所有图对，420*420
2、epoch从30000改到50
3、每个epoch都进行了验证420*140=58800
-需改进：
无
-问题：
1、原模型29000轮后的1000轮并没有使用AReg












**************** MODEL CONFIGURATION ****************
NTN_layers               -->   1
activation               -->   relu      
batch_size               -->   128       
dataset_name             -->   AIDS700nef
deepsets                 -->   True
deepsets_inner_act       -->   relu
deepsets_outer_act       -->   relu
dropout                  -->   0.0
epochs                   -->   30000
feat_norm                -->   False
fuse_type                -->   cat
gnn_encoder              -->   GIN
gnn_filters              -->   [64, 64, 32, 16]
inner_mlp                -->   True
inner_mlp_layers         -->   1
iter_val_every           -->   10
iter_val_start           -->   29000
lr                       -->   0.001
lr_scheduler             -->   False
measure                  -->   JSD
mlp_score_layer          -->   2
model_name               -->   GSC_GNN
monitor                  -->   mse
multi_contrast           -->   False
multilabel               -->   False
multirun                 -->   1
num_layers               -->   2
optimizer                -->   Adam
outer_mlp_layers         -->   1
output_comb              -->   True
patience                 -->   -1
pooling                  -->   add
recache                  -->   False
reduction                -->   2
save_best                -->   True
save_best_all            -->   True
scale                    -->   1
seed                     -->   -1
sep                      -->   False
synth                    -->   False
tensor_neurons           -->   16
use_bn                   -->   False
use_deepsets             -->   False
use_ff                   -->   False
use_mlp_score            -->   True
use_sim                  -->   True
use_ssl                  -->   True
use_val                  -->   True
val_batch_size           -->   512
val_ratio                -->   0.25
weight_decay             -->   0.0
**************** MODEL CONFIGURATION ****************